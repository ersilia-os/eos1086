{
    "Identifier": "eos1086",
    "Slug": "embeddings-extraction",
    "Status": "In progress",
    "Title": "Biomed Roberta Base",
    "Description": "BioMed-RoBERTa-base is a language model based on the RoBERTa-base with biomedical domain specific pretraining to 2.68 million scientific papers (7.55B tokens and 47GB of data). Transformer is a multi-layer structure that captures different levels of representations in different levels, it learns a rich hierarchy of linguistic information. With Biomed_Roberta_Base, we aim to extract the embeddings from the hidden states of the model.",
    "Mode": "Pretrained",
    "Task": "Representation",
    "Input": "Text",
    "Input Shape": "",
    "Output": "Other Value",
    "Output Type": "Float",
    "Output Shape": "List",
    "Interpretation": "",
    "Tag": [
        "Chemical Language Model",
        "Embedding"
    ],
    "Publication": "https://aclanthology.org/2020.acl-main.740/",
    "Source Code": "https://huggingface.co/allenai/biomed_roberta_base",
    "License": "Apache License 2.0"
}