{
    "Identifier": "eos1086",
    "Slug": "embeddings-extraction",
    "Status": "In progress",
    "Title": "Biomed Roberta Base",
    "Description": "BioMed-RoBERTa-base is a language model based on the RoBERTa-base with biomedical domain specific pretraining.",
    "Mode": "Pretrained",
    "Task": "Representation",
    "Input": "Text",
    "Input Shape": "",
    "Output": "Other Value",
    "Output Type": "Float",
    "Output Shape": "List",
    "Interpretation": "",
    "Tag": [
        "Chemical Language Model",
        "Embedding"
    ],
    "Publication": "https://aclanthology.org/2020.acl-main.740/",
    "Source Code": "https://huggingface.co/allenai/biomed_roberta_base",
    "License": "Apache License 2.0"
}